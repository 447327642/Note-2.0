# 微信

数据分析产品经理（社招），广州

部门：微信基础产品产品中心数据分析组


岗位职责:

+ 与产品经理对接各类数据需求，推动产品的发展与改进；
+ 对重点数据进行监控，能从数据异动中主动发现问题，分析和探索解决方法；
+ 对业务数据深入分析，为产品运营、业务决策和产品优化提供数据支持；
+ 对产品相关的热点问题做专题研究，收集相关数据，输出有价值的分析报告，用数据支持决策。

岗位需求：

+ 有独立解决问题的能力，能自主攻克课题；
+ 对数据可视化以及海量数据分析有丰富经验和工作热情；
+ 数据库(MySQL or Postgres)以及Hive/Hadoop实操经验；
+ 具有数据可视化软件（比如Tableau, Excel, Google Analytics）的使用经验；
+ 有较强的分析总结能力，熟练使用各种数据分析方法，可以对数据进行归纳，汇总，并进行专业细致的分析；
+ 有规划能力及意识，并且有较强的执行力；
+ 较强的服务意识，良好的团队协做能力，工作认真细致，具备高度的责任心；
+ 本科及以上学历，计算机，数学，统计，信息科学相关专业者优先；
+ 2年以上大规模数据分析经验。


## 网上面经要点

+ 看待问题的角度是否ok、分析的角度是否全面、有没有关注到细节、道理是否符合逻辑、回答的表述是否清晰有条理。

1、知己知彼。务必要了解岗位的工作内容和职责，查一下应聘部门的发展历史。起码的了解是必要的。2、是否合适。重点分析一下，为什么自己适合这个岗位，说服对方你值得聘请。3、切忌空泛。这是关键。

“校招或招实习生时，很多同学都喜欢谈行业发展趋势、战略，可是又说得很空、观点不新鲜或者缺乏充足论证。被要求谈细节或者怎么实现就傻眼凌乱了。不是说从用户角度出发不好，但同学们只习惯从用户角度说我喜欢什么什么，不会从从业者角度去多想下，或者就简单地copy媒体的观点。 还是缺乏思考啊。”“很多同学急于表现自己的见识和视野，却忽略了思考是更重要的。”一般同学在校园没有机会实际接触行业，有深切的认识不太现实。表现自己独立思考的能力，比拾人牙慧更有用。当然，这是针对校招，社招的话的确需要表现下行业视野。

产品感觉，个人经历，真诚。

聊过很多应聘产品经理的。谈下我对产品经理能力的几个层面或者档次。1.行动者 所有的成功的产品经理必须要最终落地目标,也就是执行力。这也是最基础的产品经理,理解产品完成产品上线。2.行动者+思考者 这类产品经理已经在产品层面有自己的想法和业绩,比如产品的UI布局等,用户心理等有研究能把产品层面做的很好。3.行动者+思考者+创造者这类产品经理在产品层面之上还要有创新思想,这个能力是优秀产品经理必备的.思维绝对不能被框架化。实践中就要考虑用户真实需求和市场趋势变化,并结合数据给出产品的创新东西。先给个场景模拟来形象化三类水平pd,顶头上司突然叫你拿个盒子进来他的办公室。1.此类的产品经理会直接给领导找个盒子，比如邮局给的,装零食剩下的之类2.此类产品经理会思考领导需要的盒子到底是什么样子,一问领导才知道是装垃圾的。产品就找了个上档次的垃圾箱给领导3.此类产品经理一问领导是要垃圾桶后，就在给了大气的垃圾桶后,再叫了某人几点后定时去领导办公室处理垃圾换新垃圾袋。经常在面试人中了解过往的经验来匹配下他是哪种程度。抛砖引玉。

1，逻辑性我
个人是把逻辑性放在第一位的，属于必选能力。在发掘需求时，需要通过逻辑来判断真伪。在优先级排序时，需要通过逻辑来估算价值。在和业务方、UED、开发测试进行辩论时，需要通过逻辑性来发现对方的漏洞，也需要通过逻辑性来清晰表达自己的观点。在数据分析的过程中，也需要通过逻辑性来知道要看哪些？数据发生变化的可能性是什么？怎么去验证？关于逻辑性的面试方法还是比较简单，可以针对面试者的自述项目经历，不断的追问和再追问，看看是否可以清晰地整理思路和表达。也可以问问，你和老板有没有发生过意见不统一的情况？能否回忆下细节？最后是怎么解决的（这道题很好用，也可以尝试着测试下面的第二点，同理心）也可以问问一些好玩的逻辑趣味题，不看答案，只看思考过程。如果发现对方知道答案，那么就改编问题，直到观察出候选人的思考过程。2，同理心（ Empathy）我个人觉得，如果说逻辑性是智商的主体，那么同理心就是情商的主体。同理心的重要性不言而喻。简单的问题包括，你觉得你的简历在设计上可以怎么优化？为什么？你觉得作为考官，我为什么要问上面的问题？你和你的父母曾经最大的争执是什么？你怎么解决的？（观察是否考虑到对方的感受）其他的点，例如创造与、抗压性、沟通技巧、性格态度、技术背景、时间管理能力、商业嗅觉等，我一般面试提问较少，并不是不重要，只是没有前面几点那么必要罢了

---

数据分析有很多层次，并非所有做数据分析的人都必须数理统计相关的专业出身才行。一般意义的数据分析，尤其是中小企业，传统的是对以往销售数据分析，辅助决策下个周期的采购或营销计划等。随着互联网的发展，个性化推荐与广告推送需求凸显，B2C也直接让数据的价值越来越明显，所以就有了很多数据分析相关的职位；而这些工作的胜任，只需要基础的计算和统计培训，再加上常规excel或者专业统计软件的基本用法的练习即可；相比而言对业务的理解能力更加重要，数据分析只是手段。真正数据挖掘相关的数据分析，或者说垂直领域的侠义的数据分析，就是指数据挖掘，从问题识别，建模，算法设计与计算求解，到后期 验证，对比分析，结论报告等，是将数理方法应用于实践问题的复杂过程。比如 我从事过的生物信息数据分析，面对的是ATCGAGAGAG这样的序列数据，一个人的基因组约3G，测序出来后会有N个乘数，也就是 3G x n，这是原始数据；基于此开始一系列常规的分析，然后每个细分的方向，各个小组开展各自的分析；出了常规分析，可能还需要从现有的数据中挖掘新的信息出来，尽可能地寻找与表型可能关联的“本质原因”，只有建立很多的关联可能性，才可能将基因组数据分析结论应用于个体的健康检测或疾病诊疗。这个层次所要求的不只是会使用statistica, R,SPSS,SAS这么简单(此时往往SPSS,SAS都不满足需求，大多用于工程类或实践性强的场景)，具备本科或研究生水平的数理统计专业知识背景，还要求全面的系统知识、丰富的实践经验、极强的动手实践能力，等等。 最简单的例子，你所面对的问题是：一个癌症病人，你拥有他的基因组数据，你能为他做些什么？ 虽然看起来很宏大，但实践起来也是一个一个的工作任务执行的。  又比如，我做过的一个数学建模：油田5年规划。 一个油田的产油量取决于很多因素，往往一口井开始时产量高，然后逐渐降低；油田为了稳产或持续增产，会采取N多措施。 如果在5年中合理采取N中措施，保持整个油田稳产或持续增产？已知过去的产油量数据。 这是典型的实践中的数据分析案例，结合了模拟、建模、预测、规划等问题，可以采取多种方法设计多种方案对比验证，最后给出报告。[ 实践中也确实如此，一个地方政府或部门的投资分析或财务规划就更典型了 ，类似的都属于“指定目标、优化分配资源以达成目标”的范畴。]综上，粗略地划分3个层次。LZ所谓的产品经理级别的数据分析属于level 2 。此类书籍可参考《数据分析之美》，《谁说菜鸟不会数据分析》等。



感谢邀请。最近一直在生病，基本晚上都很早休息，没有时间上线。   好吧。产品经理做数据分析，并不是从一堆原始数据里面扑拉出一堆数字，然后进行统计、分析。而更多的是在已经做好的数据上对数据与产品的关系上进行分析。   这样的分析，其实很多是现象-数据-现象这样并不复杂的联系，如果一定要每个产品经理都在数据分析上玩儿出花儿来，那让数据分析人员来做产品经理不就得了。   这里的关键在于，产品经理应该了解利用各种资源和资料，包括数据、设计、心理等各种层面的知识，帮助自己做好最重要的工作，即产品设计与运营。其他的都是扯淡。


其实我们常见的互联网数据分析，并不需要太多技术背景，更重要的是对自身业务的理解，和数据化运营的科学态度(我勒个去...)  两年前我是从程序猿转到数据分析师的，不过不是因为会数据挖掘神马的高科技，而是因为感觉没有数据支持的推广和运营太不靠谱了，大把的钱投广告，大把的钱搞在线活动，你都不知道效果怎么样，心虚啊。。。  我理解的数据分析师，应该是了解业务，从业务需求出发，衔接数据挖掘等技术团队，获得所需的数据，通过自身对业务的理解得到答案，为相关业务部门提供指引的。  我们会发现这个角色并不需要太多技术基础，而是要深入的了解自己的产品。知道网站的流量都分布在哪些产品、每个产品的流量又分布在哪些功能和页面，要知道网站的流量来源、不同来源的用户行为有什么区别、喜欢哪些功能和内容。。。有了这些了解，他看到数字以后才能告诉你，数字背后意味着什么，让数据发挥它的价值。  然后我们回过头来看，上边那堆东西，不正好是一个产品经理应该了解的吗:)


仅对技术的要求来看1.查数 = 数据分析（知道pv,uv,点击流，能写sql和脚本扫log）2.在1T的log级别上查数 = 大规模数据分析，用户行为分析（要用hadoop了）3.用svm,朴素贝叶斯，knn一类的，不能叫数据分析了，叫数据挖掘了（R，matlab，python科学计算等）技术其实不重要，关键是对业务的理解


产品分析生物链：   1.产生需求-2.整合需求-3.数据实施-数据分析（报表+深度分析）-4.分析结果传递-5.产品经理应用-6.数据应用反馈；   整个过程中2、3是数据分析专业人员做的，但需求的提出、整合、沟通、应用反馈几乎是产品经理主导的，没有靠谱的需求就没有靠谱的分析更不可能有靠谱的应用了，所以：    1. 了解数据的基本概念与原理，因为数据也会撒谎，同一纬度的统计不同会产生完全不同的结论；   2.多看数据，数据的感觉与产品和其它艺术一样看多了培养起来的；   3.与分析师多沟通需求，把需求弄清楚细分挖掘比做一百个似是而非的需求强，宁精勿烂；   4.清楚业务之间的逻辑关系，数据从来不是独立的需要不断的看各数据间的关系，找出规律得出结论，有时候逻辑错了，方向就反了；   5.不要迷信数据但也不要轻视数据，欠久了基础到时候需要的时候可没那么好补；   6.数据分析是把那些隐藏的你看不到的真象给清楚的表达出来，是高效直接有力的，如同神经网络一样，数据分析不断的沟通过程是学习的过程，比较成功的分析结果是这个学习过程是成功的，最终的预测结论便是成功的，这个学习过程无逻辑可参考性低当然预测也就失败了。


产品经理只要清楚自己想知道哪些数据之间的什么样的关系就可以了，并能够准确表达给专门的数据分析师，由他们出报表。   人的时间是有限的，数据分析也不是产品经理的主要工作，所以不能要求自己对这方面很专业。即使没有专业人员帮你，玩好google analytics也够用了

---

现在人数在50人以下的小组大约占了全部4000+个小组的90%，那么按照豆瓣的分布，这后90%的小组全部加起来一共有多少的出现几率？（在全部六个位置中）即我们有多大机会在“15分钟名组”看到一个不到50人的小组？10%？5%？1%？还是更少？

提的比较清楚的是后一个问题。重述在此:

在”15分钟名组”里看到（至少）一个不到50人的小组的几率有多少?

这其实是一个统计的问题。可以去做一个模拟，不过我想看看有没有更聪明的答案. 所以在这里公开征解。

先看看”15分钟名组”是怎么来的。豆瓣的FAQ说:

[6个"15分钟名组"]从所有小组里随机挑选。每15分钟更新一次。这个列表是为了打破以前“最受欢迎的6个小组”造成的贫富分化日巨问题。

但是不是所有小组都机会均等（这样的话，选中的小组基本上会只有一个成员）。小组成员越多，选中的机会越多。挑选是由豆瓣系统自动用随机数进行。所有小组按人气排队以后，选中的几率遵从一个长尾分布。所以即使只有一个人的小组也可能上榜，但极有可能是一个月里某一天的凌晨3点52分。

这个“长尾分布”, 豆瓣的程序里是用Pareto分布计算的。学统计的，或者看过Linked这本书的人应该知道。定义在这里:

http://www.answers.com/topic/pareto-distribution

豆瓣现在用的k=0.5 (就是Pareto Index, 也叫alpha). 排序(x)从1开始。此刻现在共有4154个小组，第421个小组以后成员数就小于50了.

第一个模拟出来的正确答案的，我请喝酒。第一个给出解析正解（就是用公式算出来）的，我送超女之一的亲笔签名T-Shirt (空白T-Shirt自备)。我还不知道正确答案，所以可能需要两个以上正解。

没准这个问题有一天会出现在研究生入学考试题里。:)


---


偶同样文科出身，对高级的数据挖掘各种算法基本白目，但做了一段时间的PM偏数据工作，谈谈几点感想：1. 了解业务，熟悉数据框架、体系了解你的业务是做什么的，业务的发展规划有什么，衡量的核心指标有哪些，列出KPI或是核心指标，一般重点指标就那么几个；然后对几个核心指标进行拆解，这点也需要根据你的业务属性进行，你的业务凡是会影响到这个指标的有哪几个元素，举个简单的例子，你开发的手机APP上的登录次数、用户数，你拆分为是ios、安卓、wp7还是其它，如果你是接入的新浪或者QQ的开放平台账号，拆分为新浪账号、QQ账号、人人网账号或是单独注册等一系列；拆分的好处是你能对一个具体的指标很清晰它是怎么组成的，好像庖丁解牛，当然这个过程可以不断拆分下去，加上一些公共属性，例如时间、用户性别、用户年龄、用户职业等公共的纬度进去细切，不过个人觉得这是建立在有专人专团队做这事的基础上去不断细分数据，这些结果可以帮助你更精准的定位你的产品，为你后面的运营、推广、品牌等定位出一个比较精准的模型；2. 对现有数据指标进行思考；多维度集中分析找规律在熟悉产品需要关注的指标、框架之后，了解现有每个指标的运营现状；如果有同行业指标对比更好，看是否有提高的空间；或者是，希望通过某个运营的动作，提高哪一个指标，提高到多少；回到1中的例子，例如发现安卓的女性用户偏高，且登录时间集中在周五晚上，那下一次如果做活动运营的内容可能有所偏重，同时发布时间也尽量靠近那个时段；通过一系列的比较精准命中，预估运营能够提升指标到一个什么水准；另外有一个精准模型的好处是了解你的核心用户后，你可以单独针对这部分用户进行产品用研与需求挖掘，更利于你内心确定哪些指标是可以通过什么手段提升的；同时找规律，对于拆解出来的指标，想办法做一些分析，这里的分析个人觉得并不一定需要很复杂的手段，更重要的是一种感觉和意识，可能觉得某些数据之间会关联，例如可能登录高的时候用户的UGC内容产生频率也很高（这种简单的不能再简单的相信一般人都会有意识=。=容我举个这么简单的栗子），这里的例子是很明显易见的关联，但其实有很多数据之间的关系可能没那么明显，需要一种直觉去组合，然后判断，如果不是通过归纳分析能得到结论的，甚至可以想办法去做一些改动证明；3. 规律验证，经验总结找到规律了，内心明白了，下一次做事情心里会更透亮一些，对产品的理解又会更深一些；很多事情，就是这样一点点去熟悉，去深入慢慢产生亲切感的；数据是让你和你的产品心灵贴近的一个话题而已，聊到一定程度对于PM来说就差不多了，可以聊些别的了；更高级更深入人心的数据沟通，不妨还是交给专业的数据处理人员吧，就好像不是人人都是心理咨询师一样；总之，对于PM而言，个人觉得数据是一种意识，而非技术，是一种方法总结，而非理论科学，关注数据是个优点；但PM也还有很多别的事儿要干呢，PM的角色不就是%@#￥%，不是么....

---

著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：林通
链接：http://www.zhihu.com/question/20134667/answer/14165215
来源：知乎

每个产品经理在产品设计前就需要明白一个最简单的公式：产品价值=产品带来的收益一设计研发运营成本>0例如积分类的产品,如果使用了积分产品后净增的销售额*利率-积分充抵的商品价值（运营成本）-设计研发成本>0，如果用户会长期使用积分，设计研发成本可以忽略，其它数据可以比较容易拿到。再如页面改版类产品，改版带来忠实用户数*每忠实用户价值-新页面的设计研发运营成本>0，说明改版是成功的。产品经理只要把握好这个基本公式，其其深入的数据分析交给更专业的人员去做吧，产品经理的主要精力还是放在用户需求分析层面。

---


. 对于大多数网站，如果你想用数据为导向，必须建立系统级的A/B测试机制。对于界面层面的重构，一个产品经理+一个工程师，一天用这个系统一天至少能做3-4个。系统级别的A/B测试要能够保证快速上线，第一时间看到数据，一旦超过临界值直接结束测试、保留数据并生成报告（直接邮件发送，而不是让产品经理想起来跑到后台再查）2. 对于做社交网站，或者有复杂用户数据模型的公司，要在界面呈现和用户数据之间建立匹配系统。这样产品经理可以设计几种呈现模式，丢到匹配系统中，过不了多久，就能发现用户对不同呈现的数据反映的不同，然后系统性地固化这种机制。3. 通过cookie或者用户登录信息，建立针对不同用户的内部tag系统，看这些tag在系统2里有没有明显差异。如果有就可以固化下来，用来提高关键指标。所以，我现在对于数据分析的感觉是：1.要提高一个数据指标，盯着它是没有用的。必须找到影响这个数据的另几个可操作性更强的数据指标，调整它们。2.分析数据的可能性要充分，充分分析的基础是测试充分多的可能性。如果你想测试图标的颜色从绿色变成红色会不会更好。那为什么不测试一下蓝色，紫色和黄色呢？3. 如果小规模数据已经可以说明问题，就没有必要延长测试时间，也没有必要扩大测试范围。4. 要充分利用计算机来帮你做数据采集和分析，缩短数据分析的周期，降低数据分析的成本。5. 有必要的时候，可以让计算机帮你找pattern，因为计算机没有偏见。

---


数据分析是一种靠谱的产品研究方法, 这玩意有很多误区, 也不能迷信, 最终到头来还是要人来做决策忽略沉默的用户二战时英国空军为了降低飞机的损失，决定给飞机的机身进行装甲加固。由于当时条件所限，只能用装甲加固飞机上的少数部位。他们对执行完轰炸任务返航的飞机进行仔细的观察、分析、统计。发现大多数的弹孔，都集中在飞机的机翼上；只有少数弹孔位于驾驶舱。从数据上说, 加固机翼的性价比最高. 但实际情况缺恰恰相反, 驾驶舱才是最应加固的地方, 因为驾驶舱被击中的飞机几乎都没飞回来."发声"的数据是最好获取的, 但如果没把这些沉默的数据考虑进来, 那么这种数据分析是不靠谱的. 所以除了数据的结果, 还得尝试解读这些数据. 而解读数据就完全依赖人了. 把沉默用户当做支持和反对的中间态2家网站A和B，都经营类似的业务，都有稳定的用户群。它们都进行了类似的网站界面改版。改版之后，网站A没有得到用户的赞扬，反而遭到很多用户的臭骂；而网站B既没有用户夸它，也没有用户骂它。如果从数据来看, 应该是网站B的改版相对更成功, 因为没有用户表达不满。但事实并非如此。网站A虽然遭到很多用户痛骂，但说明还有很多用户在乎它；对于网站B，用户对它已经不关心它了.网站A指的是Facebook，网站B是微软旗下的Live Space。把数据作为决策的唯一标准通常认为数据分析指导工作是一种高性价比的做法, 不容易犯错, 对于代表资方的管理层来说, 比起依赖于人的决策, 依赖于数据的决策似乎更稳健. 这种决策在从0.5向0.8的产品改进上, 可能是有效的. 因为一个已有的产品, 数据就摆在那. 100个用户50个访问超时, 解决了这个问题, 就提升了50%的效果.但对于从0到0.1的新产品上, 由于数据很难获取, 需要花大力气在获取模拟数据上. 往往是用一周时间去想明白一个做两个小时的产品该不该做的问题. 而且模拟的结果还和最终实际相差很远.A/B test或是原型系统, 先做出来, 再去验证, 在一些场合下比先拿数据要有效的多.认为数据是绝对客观的为了减少内耗, 往往依赖于数据来做决断. 我一直认为数据本身是带有主观性的, 完全客观的数据是没有的. 数据的获取方法, 数据的解读方法, 数据的统计方法, 都是人的决策. 一份数据拿出两个相反的结论来也不是没有可能. 即使主观上没有偏向性, 也受限于方法和视野. 决策上最终起作用的还是人不是数据. 虽然人有那么多的不确定性, 还可能出现争论, 扯皮, 不敢承担责任.

---

首先数据作为一种重要的评估手段被拿来说事是经过很多年验证的，但数据本身是死的，需要我们去分析，如今数据是越来越多了，但是分析的结果却越来越低了，显然这是人的因素，一个pm能够拿着数据让rd干活必须做到几点：1.   对数据有客观的分析过程，最终产出的不应该只是一个结论，还应该包括你的分析方法和分析过程，这些也要讲给RD，只有这样数据才真正起到了应有的作用！从这个角度讲为什么使用4星，而不是3星或者5星，也应该是给出理由的，而且必要时也需要数据证明。2.  不要对数据想当然，很多pm拿到数据根本没做深层次的分析就直接告诉RD你看这个数据说明什么的，甚至其他网站的运营数据，很多PM会直接拿来讲给RD，这个情况我相信即使资格深一点的PM也有的。我要说的是不要把其他人的数据想当然成我们自己，一个产品能够成功是有个过程的，这就是我经常说的成长期，我们虽然没办法参与每个过程，但是在使用别人数据的时候还是需要分析这个过程的，数据不能脱离年代、环境、用户、等等很多客观因素单独存在！3.  不要过于依赖数据，数据只是我们产品发展中辅助的东西，PM应该更多的了解很多数据之外的事情，如社会环境、用户水平、竞品状态，这些东西新人更应该注意，不要总拿个数据跟RD转牛脚尖！其次，作为RD我觉得我们一定要在工作过程中变得更加主动！1.  如果对PM的分析结果或者分析过程不满意，要随时提出，不要太被动，太被动你就慢慢成了被影响的人了，对自己前途，产品前途都不利！2.  主动去跟PM沟通协调，其实大家目标是一致的，既然这样，我们都是追求最大的效率，你要把你的实际情况反应给PM，让大家都能去权衡得实！最后，我个人觉得RD别总太转进技术中，也要学习新的东西，包括产品设计，项目管理，敏捷开发，很多看似不相关的东西往往是相关的，对自己影响很大的！你提到 “实际上，该功能整个平台的用户都希望做，是没有必要耗费人力评估的，只要做就可以了”其实你自己的想法是存在问题的，第一，你怎么了解用户的想法的？第二，该功能到底是基础功能还是点缀的功能？第三，眼下什么是最重要的？我相信评估也许是必要的！

---


近期，http://OkCupid.com(一个交友网站)做了一个测试，把首页的最近来访列表移除了，只留了一个不显眼的入口。注意：最近来放列表对99%的交友网站都是必备！！！有人在Quora上问为什么会做这个测试，他们的产品Tom Jacques这么回答：1，OkCupid很依赖数据，这些都是做A/Btest的结果2，数据显示，如果在首页放置最近来访，这个功能会和背景融为一体，用户会更少的点击，而且进入个人资料后，发消息也少了。反而，将入口藏起来，用户会更严肃的对待这个功能，点击率和消息率都增加了。我又追问了2个问题，这哥们的答案能看出他们做事的方式：Q：怎么证明这不是一个短期效应，比如用户对改变敏感，时间长了这个行为又变回去了。A：测试会持续足够长的时间。同时，对新用户进行测试，这部分用户没见过之前的形式。这两个措施保证了可以提高测试的可信度。Q：最近来访看起来是个常规功能，而且看上去很不错，怎么想到对这个功能进行A/B test。A：我们的原则是怀疑一切，有些功能看起来不错，但是不是说他不能做的更好。所以，我们经常性的对一些功能进行测试，确保他们是不是真的最好。最后介绍下OkCupid这个交友网站，创始人是哈佛几个学数学的人，通过各种测试帮你找到合适的人。


---

数据是一种信仰毁掉分析数据态度的三个常见原因 首先，大环境不尊重数据，尤其是老板的态度。如果数据分析师只要随便给一个报告就行，数字多一点和少一点，大家也是一笑而过，并不会追根到底，那么很难让数据分析师以严谨的态度对待数据。例如，国内这几家数据分析机构，基本都在着急扩张行业，争着占领行业，对于其推出的数据有多精准却不那么在意，所以艾瑞的数据最近才会经常被人说“不靠谱”。数据分析，今天做得不准，明天再改是没有用的。比如艾瑞，如果数据不稳固，抢着做很多行业，这是不靠谱的做法，指不定哪天砸了自己的牌子。有人和我提过FACEBOOK数据分析师为什么那么牛，因为他们不觉得数据分析是一个苦事，十几个人在一个房子里把数据分析当做一件很开心的事情来做，数据分析对于他们来说是在追求科学。第二，好的数据分析师需要一点天分，同时也需要高人点拨，但是电子商务这个圈子，真正懂数据分析的人不会超过10个，所以一般人很难取得真经。这和信仰一样，没有师傅领进门，难度也会大很多。我回顾自己从微软到易趣，再从敦煌到支付宝，在数据分析上有一次长足的进步，得益于从两位老师的身上得到了许多启发。一位是亚马逊的首席科学家韦思康，曾经，我告诉韦思康，KPI报告显示敦煌网需要4秒钟，他立马让我叫来做技术的同事(他要听到一线同学的反应)，问这个4秒钟怎么测算出来，是美国人打开用4秒钟，还是英国人打开用4秒钟，用的是甚么Browser等等。这个4秒钟和商业价值(例如交易量)有关系吗？我当时很触动，连这么一个很基础的数据，他都是以求证的心态来分析的。更令我印象深刻的是，只请他当敦煌网顾问半天，按照他的工作经历来说，随便忽悠我半天是很容易的事情，但是韦思康非常严谨，先是以一个普通人的身份花了半个小时在敦煌网买东西(坚决要真实付钱)，切身体会敦煌网的用户体验，然后也不先看数据，而是先问很多能更了解敦煌网的生意形态的问题。讲真他的问题比很多投资分析师来得专业。而现在许多数据分析师，包括当时我自己，只看数据就开口说问题，不深入去体会公司的商业形态。韦思康告诉我数据是一种态度，让我明白做数据的人就是要全身心投入，好像一种信仰一样，中间有许多路要走；而且，数据与商业密切相关，不能局限在数据的死角里。另一位是清华大学的教授谢劲红，有一个夏天碰巧去旁听他的课，拿一堆的数据给他看，他一边看一边给我演绎他的思维，他可以很快在一堆数据找到他们之间的关系。后来我带着团队常常去清华找他聊，他教我如何看网络数据，用联动的思维来看网络数据。可以说是他启蒙了我用 “关系”的思维看数据。一听完就回到敦煌跑到敦煌看很多数据，发现了新世界。第三，数据分析师感叹落不了地，只能谈数据，而不懂商业。如果不懂商业，而单纯看数据，不仅很难有创意的思维，而且是没有意义的（曾经谈过这个问题，不懂商业就别谈数据：http://blog.sina.com.cn/s/blog_5025e3880100kwn1.html）。而对于一般的数据分析师来说，大部分人没有系统思维，而且也只能看一部分数据，无法从大面儿上了解整个公司的运营数据，这样就令数据分析师难以形成全面的思考方式。以我自己的工作经历来举例，为什么我在敦煌的时候数据分析能力会突飞猛进，也是因为我在前两家公司只能看到一部分数据，而到了敦煌之后我爱看什么就看什么，受谢教授启发之后我更是天马行空地把营销数据、市场数据、财务数据、产品数据、卖家和买家数据等等联动起来看，这大大改变了我对数据的运用方式。

---

著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：徐晓峰
链接：http://www.zhihu.com/question/19615108/answer/12877621
来源：知乎

本人常年从事市场产品工作,对数据统计自学过一阵为硕士论文用。就我自己体会讲几点：1、有意义的数据极其有必要，这里的有意义指a.精度有意义，精度太高很不必要代价太大精度低了那和没有数据也是一样。b 指向有意义，这个就需要丰富的经验来把握，哪里去获得数据？和谁去比较？能说明什么？这一系列问题能把握住才能称为数据指向有意义。2、不赞成先有结论再去用数据求证。提出这中说法的人基本上都是用来凑数据的。稍微了解统计思想的就知道，统计的证明是建立在拒绝基础上的，而不是承认假设。不知道有多少人想过没有这是为什么？不展开了。3、PM的黄埔军校是快消行业（鄙人是搞通讯的，水平差很多），快消行业不用数据说话那简直。。。这里肯定有人说我是搞IT的通讯的B2B的，和快消差很多，我们行业关注的是决策链等等。我要说这都是扯淡，举个例子微软现在的CEO原来就是在宝洁干PM的，所以洗发水和软件本质没啥区别4、在中国的传统里理数被认为是奇技淫巧之类因此是欠缺的，所以中国人的严谨性群体缺失我敢说坚持数据无用论中90%以上是懒得去搞数据，或者看见的垃圾数据太多已经失去对数据的信赖，但我相信一旦一份逻辑严密数据严谨的数据放在你面前你一定无法忽视。5、人的辨别能力是有限的，很多数据工作是为了研展你的辩识能力，比如数据挖掘。一堆杂乱的数据对你是毫无用处的，但通过处理就可能告诉你很多你忽略的信息。因此数据挖掘我认为是PM必须具备的职业技能。补充6、最近看到了一篇讲数据统计方法的文章全文如下，很纠结转不转有点长，但想想来知乎的应该都不怕长的，就转了吧。如果你能搞明白“因果网络”这个关键点那么数据分析就有极有意义的，如果你搞岔了那么数据分析会误入歧途。当然任何项目的展开都受制于资源，但这个思维方式在数据分析前必须具备的，全文如下：-------------------------------------------------------------------- http://www.matrix67.com/blog/archives/930 （原文地址）zhihu网上曾经有过一个问题，谈如何看待用数据说话来分析问题，看了很多很多回答，自己也尝试回答了这个问题，看见这篇文章才知道真正的问题所在就是“因果网络”-------------------------------------------------------    在去年10月份的数学文化节期间，我去听了好几次讲座，其中有一些讲的相当精彩。时间过得好快，转眼间又是一年了，如果不是Wind牛发短信问我去不去听讲座，我估计今年数学文化节过了都还想不起这档子事。于是和Wind牛跑去二教309，听了一场叫做《从数据中挖掘因果关系》的讲座。这个题目是很有趣的：数据本身并不说谎，难就难在我们如何从中挖掘出正确的信息。当我们讨论数据时，我们讲的最多的是数据的相关性，而我们希望得到的则是事件之间的因果联系；但事实往往是复杂的，统计数据有相关性并不意味着两个事件具有因果联系，而具有因果联系的两件事从统计数据上看有时也并不相关。    对于前者，最简单的例子就是公鸡打鸣与太阳升起：公鸡打鸣与太阳升起总是同时发生，但这并不表示把全世界所有的公鸡都杀光了后太阳就升不起来了。统计发现，手指头越黄的人，得肺癌的比例越大。但事实上，手指的颜色和得肺癌的几率之间显然没有直接的因果联系。那么为什么统计数据会显示出相关性呢？这是因为手指黄和肺癌都是由吸烟造成的，由此造成了这两者之间产生了虚假的相关性。我们还可以质疑：根据同样的道理，我们又如何能从统计数据中得出吸烟会致癌的结论呢？要想知道吸烟与癌症之间究竟是否有因果联系的话，方法很简单：找一群人随机分成两组，规定一组抽烟一组不抽烟，过它十几年再把这一拨人找回来，数一数看是不是抽烟的那一组人患肺癌的更多一些。这个实验方法本身是无可挑剔的，但它太不道德了，因此我们只能考虑用自然观察法：选择一些本来都不吸烟的健康人进行跟踪观察，然后呢，过段时间这一拨人里总会出现一些失意了堕落了犯上烟瘾的人，于是随着时间的流逝这帮人自然而然地分成了可供统计观察的两组人。注意，这里“是否吸烟”这一变量并不是随机化得来的，它并没有经过人为的干预，而是自然区分出来的。这是一个致命的缺陷！统计结果表明，犯上烟瘾的那些人得肺癌的几率远远高于其他人。这真的能够说明吸烟致癌吗？仔细想想你会发现这当然不能！原因恰似黄手指与肺癌一例：完全有可能是某个第三方变量同时对“爱吸烟”和“患肺癌”产生影响。1957年，Fisher提出了两个备选理论：癌症引起吸烟（烟瘾是癌症早期的一个症状），或者存在某种基因能够同时引起癌症和烟瘾。    有虚假的相关性数据，就有虚假的独立性数据。“健康工人效应”是一个特别有意思的理论。调查发现，在铀矿工作的工人居然与其它人的寿命一样长（有时甚至更长）。这表明在铀矿工作对身体无害么？当然不是！其实，是因为去铀矿工作的工人都是经过精心挑选的身强体壮的人，他们的寿命本来就该长一些，正是因为去了铀矿工作才把他们的寿命拉低到了平均水平。这一有趣的细节导致了数据的伪独立性。类似地，有数据表明打太极拳的人和不打太极拳的人平均寿命相同。事实上呢，太极拳确实可以强身健体、延长寿命，但打太极拳的人往往是体弱多病的人，这一事实也给统计数据带来了虚假的独立性。现实中的统计数据往往会表现出一些更加诡异复杂的反常现象。Simpson悖论是统计学中最有名的悖论：各个局部表现都很好，合起来一看反而更差。统计学在药物实验中的应用相当广泛，每次推出一种新药，我们都需要非常谨慎地进行临床测试。但有时候，药物实验的结果会让人匪夷所思。假设现在我们有一种可以代替安慰剂的新药。统计数据表明，这种新药的效果并不比安慰剂好：         有效   无效   总人数新药      80    120     200安慰剂   100    100     200    简单算算就能看出，新药只对40%的人有效，而安慰剂则对50%的人有效。新药按理说应该更好啊，那问题出在哪里呢？是否是因为这种新药对某一类人有副作用？于是研究人员把性别因素考虑进来，将男女分开来统计：       男性有效  男性无效  女性有效  女性无效新药      35        15        45       105安慰剂    90        60        10        40    大家不妨实际计算一下：对于男性来说，新药对高达70%的人都有效，而安慰剂则只对60%的人有效；对于女性来说，新药对30%的人都有效，而安慰剂则只对20%的人有效。滑稽的一幕出现了：我们惊奇地发现，新药对男性更加有效，对女性也更加有效，但对整个人类则无效！    这种怪异的事屡见不鲜。前几个月一个高中的师弟给我发短信，给了我两个大学的名字，问该填报哪个好。鉴于我目前的悲惨境遇，我非常认真地帮他查了一下两所大学的男女比例，并且很细致地将表格精确到了各个院系。然后呢，怪事出现了：A学校的每个院系的女生比例都比B学校的同院系要高，但合起来一看就比B学校的低。当然，进错了大学找不到MM是小事，大不了像我一样20岁了连初吻都还没有，拿出去丢丢人让别人笑话笑话就完事了；但医药研究需要的是极其精细的统计实验，稍微出点差错的话害死的可就不是一两个人了。上面的例子再次告诉我们，统计实验的“随机干预”有多么重要。从上面的数据里我们直接看到，这个实验的操作本身就有问题：新药几乎全是女的在用，男的则大都在用安慰剂。被试者的分组根本没有实现完全的随机化，这才导致了如此混乱的统计结果；不难设想，如果每种药物的使用者都是男女各占一半，上述的悖论也就不会产生了。当然，研究人员也不都是傻子，这么重大的失误一般还是不会发生的。问题很可能出在一些没人注意到的小细节上。比如说，实验的时候用粉色的瓶子装新药，用蓝色的瓶子装安慰剂，然后让被试人从中随机选一个来用。结果呢，MM喜欢粉色，选的都是新药；男的呢则大多选择了蓝瓶子，用的都是安慰剂。最后，新药和安慰剂都发完了，因此直到结果出来之前没有人会注意到这个微小的性别差异所带来的统计失误。    当然，上面这个药物实验的例子并不是真实的，一看就知道那个数据是凑出来方便大家计算的。不过，永远不要以为这种戏剧性的事件不会发生。一本叫做《致命的药物》的书详细披露了20世纪美国的一次重大药害事件，其原因可以归结到药物实验上去。药物实验的时间是有限的，如果用死亡率作为唯一标准的话，估计每个药物实验都得观察个十几二十年才行。为此，科学家们想到了利用各种“中间变量”来替代死亡率这一指标。    统计数据表明，抑制心律失常能够减少死亡率，而当时的药物实验明确表明该药物能有效地抑制心律失常。这些药物得到了FDA批准并成功上市，当时每年有20多万人服用这些药品，超过5万人因为服用这种药物而死亡。这个药物实验中蕴含的逻辑推理看似无懈可击，到底什么地方出错了呢？人们推测很可能是某个第三方变量的问题。我们不妨称这种情况为“中间变量悖论”。               抑制心律失常           死亡率              对照组  实验组     未抑制    抑制无缺陷(70%)    0.02    0.99       0.02     0.01有缺陷(30%)    0.98    0.79       1.00     0.02---------------------------------------------------期望值         0.38    0.93       0.314    0.013    让我们假设存在一个第三方因素，例如基因问题。我们不妨暂时管它叫做“先天缺陷”。从上表中我们可以看到，实验组（使用新药的人）中有93%的人成功抑制了心律失常，远远高于什么都不做的人(38%)；同时，心律失常确实会导致31.4%的人心脏骤停而死，但抑制心律失常则把这个比率下降到1.3%。这似乎确实可以说明，新药能够有效降低死亡率。但引入第三方因素后，情况有了很大的改变。有先天缺陷的人，心律往往很正常，恐怖的是一旦无法抑制心律失常则必死无疑。真正要命的就是，这种药物会使那些有先天缺陷的人心律变得更差。在没有缺陷的那70%的人当中，用药后有99%的人能抑制心律失常，而这里面只有1%的人会死；同时，另外1%的人则无法抑制，其中又有2%的人会死亡；有先天缺陷的那30%的人就惨了，用药后抑制住心律失常的人反而下降到79%，其中有2%的人会死，而对于另外21%的人则必死无疑。计算表明，使用药物后死亡的人数竟然三倍于不使用药物时的情况！   (0.7*0.99*0.01 + 0.7*0.01*0.02 + 0.3*0.79*0.02 + 0.3*0.21*1.00)/ (0.7*0.02*0.01 + 0.7*0.98*0.02 + 0.3*0.98*0.02 + 0.3*0.02*1.00)≈ 2.91    可以看到，从数据中挖掘因果关系并不是那么简单的事。如何确定影响目标的事件，如何从数据中获取相关关系，怎样用最少的实验次数（控制最少的变量）为因果关系定向，这都是建立一个因果网络所需要考虑的因素。因果网络是一个很复杂的学问，前天的讲座里还提到了很多确定因果网络的算法，在这里我就不再多说了。

---


对数据的使用程度，取决于你的产品所处的阶段。当你的产品当前处于初级阶段时，你的产品其实是“一切皆有可能”套用一些数据分析方法，反而束缚了产品可能发挥的空间。这个阶段更依赖初期做这个产品的冲动和对用户需求的感性把握。当你的产品当前处于用户基数很大且成熟期时，你的产品已经确确实实满足了用户的某种需求，这个需求的满足是立足点，不能轻易动摇，而且需要进一步完善和改进，这个时候数据的分析很重要，也更加客观。关于数据分析和用户反馈，一篇社交游戏设计的文章中提到的观点很有借鉴意义，以下为引用请参考：很多人社交游戏公司是依赖数据分析来判断用户喜好并不断进行修正，但事实上类似Zynga这种拥有超级用户数据分析能力的公司并不多见，大部分公司根据用户行为调整游戏方案一直停留在纯理论阶段，我见过部分公司对市场反馈不理想的游戏直接做法是回炉重新打造。对于缺乏资本和资源支持的公司，最好的做法是预判用户需求而不是试图实现用户的想法。数据分析是一个很传神的说法，但是这个更多只限于细节方面的调整，就连Zynga在制作Cityville的时候也有类似的困扰，他们刚开始并不清楚Cityville会不会只是Farmville的简单翻版，甚至还打算让Challenge Games来接手这个项目，但最后在团队的磨合下才形成现在的游戏样式和超级影响力。Cityville的最终成型和用户的反馈关联不大，最核心的部分还是在于开发者层面的自我调整。It Girl设计师Brice Morrison认为数据分析是根据用户反馈来反推游戏设置合理性，比如每个等级的玩家数量、体验时间和消费金额。还有另外一个重要层面是玩家的游戏进程中所处的量级并不一样，开发者利用了玩家的时间差，以少部分领先者为体验对象，再根据他们的反馈修正并服务于后面绝大部分的玩家。Brian Reynolds提到过他们是如何在FrontierVille中从马蹄铁的提供数量来判别玩家对一些微调的反应。


---


数学上，关于“风险性决策的最优方案”模型的逻辑用来说明这个问题会比较简单。（一）样本的采集与处理。第一类，采集的样本量的问题。主要是样本量不够全面（包括样本数量不够大以及样本不具备足够的代表性），导致数据不可信。这种情况，我把其分为两种相对“极端”的理论：其一是黑天鹅理论，其理论的基本依据即是：我们所有的结论都是基于先前已有的经验，这样很容易让人忽略之前没有出现过的现象。即无论样本量多大，考虑到时间空间的无限性，样本量总归是相对不够的。这个理论的例子可以用哲学上比较有名的那句话：“你不知道明天太阳是否还会照常升起”来加以说明。另外一种，我称之为伯格森理论，他认为科学对生命的描述是分裂的，比如我们会描述性格、描述外形、描述智商，当时一切的描述都是片面的属性，而人是整体的，所以所有对生命的描述都是旁观的，不全面不准确的。另外，生命是变化的，而理性的描述是静止的，所以生命无法描述。出生于尼日利亚的小说家Chimamanda Adichie在TED上有一个讲座，题目就是《单一故事的危害性》，她讲到自己从小读国外的书籍，发现里面的主人公都是白皮肤蓝眼睛，以至于她一度认为只有描写外国人以及那些她不曾亲身体会的生活才算得上是写作，直到她接触到了非洲文学，才发现原来像她一样拥有卷发的巧克力肤色的人物也能成为小说中的存在她刚去到美国的时候，她的室友惊讶于她标准的英文发音，却不知道尼日利亚的官方语言就是英语；她的室友想听她们的部落音乐，所以当她拿出流行歌手CD的时候对方感到无比震惊；她的导师说她写的论文没有非洲的真实性，仅仅因为里面的人物没有面临饥荒，而且还会开车。正如她所说的那样，单一故事通常不是不对，而是不全面，因为它让单一的故事代表了一群人。同样，数据采集也面临这样的问题，采集的数据通常不是不对，而是不全面。而人类本能地过多看中容易获取的数字，而低估难以获取数据的价值，也通常会造成样本量不全面的问题。第二类是数据预处理的问题：1、简化或者扭曲某系相关因素。关于简化，最常见的经济学上的“不考虑交易成本的完全竞争理论”；而关于扭曲，最常见的就是将“相关性”与“因果关系”混淆，得出类似"拥有自己独立办公大楼的公司更容易成功"的结论。2、相关因素难以量化的问题。比如一个公司“创新能力”“人力资源管理”对公司业绩的贡献度问题。（二）模型的构建与评估第一步：考虑一个涉及风险性决策的问题时，我们首先需要一个效益函数，简化起见，我们用线性的函数来大致的描述：这就引出3个重要的内容：1、我们的目标Y是什么2、影响目标Y的各种因素是什么3、各类因素对Y的影响权重是多少在进行到这一步的时候，人们如果仅凭经验，就会有很多人犯错了：1、他们只看到了某一个影响因素，而忽略了其它因素的影响。（人类总是会有意无意忽略一些影响因素，而刻意留下对自己观点有利的因素）2、他们给予某一个影响因素过多的权重。（人类的记忆倾向于给最近发生的事情、印象最深刻的事情赋予过大的权重，上面有人提到的给“飞机机舱加固”的案例其实就是权重问题）3、目标Y的确认也是会存在出错的可能，因为现实生活中，我们的目标通常不是唯一的，比如把“用户体验”当做目标的时候，通常还需要考虑我们的预期盈利情况、市场定位的情况。这一类的错误非常常见，只见树木不见森林。具体到您这个例子中，您提到的“对于某项目,PM凭经验可说4级以上的用户可xxx，这时候会有人跳出来问，为什么不是3级、5级？拿出数据来。 实际上真看了数据又能看出什么呢？看完后无非是再次验证了4级。”如果您不“全面”地看数据，那么您就可能只去寻找对自己有利的数据，或者过多看重自己的偏好数据，以及可能忽视了与其他目标存在冲突的情况，也就是上述的3个错误。进一步的，通过数据分析，即便与您最初预想结果一致，您也会更加清楚地明白每一个影响因素是什么，每一个影响因素的实际贡献是多少，这样的过程才能让您的逻辑更加无懈可击。第二步：考虑风险因素，也就是概率问题。为了避免引入过多的数学公式，在此不展开概率矩阵，大致说明一下：我们可以把：看成一种概率下的函数；同样的，在其它的概率条件下还存在着很多的的函数。进行到这一步的时候，又会有很多人不经意间犯错：1、人们通常会选择性地只看到对自己有利的概率事件，忽略对自己不利的概率事件。具体到您这个案例上，您提到的“ 再比如有的功能是肯定要上的，但领导会说，调研一下有没有必要，评估数据搞半个月，评估的结果是：可做。 实际上，该功能整个平台的用户都希望做，是没有必要耗费人力评估的，只要做就可以了。”那么，您非常可能就犯了上面的错误，忽略了增加该功能所可能带来的不利的情况。而要客观评估这种情况，就需要考虑到风险因素。进一步的，通过数据分析，即便与您最初预想结果一致，您也会更加清楚地知道改动之后所面对的最大的损失会是如何，令您的决策更加有备无患。第三步：考虑可执行性。这一问题在简化的数学模型上通常不会被提到，但是在现实世界中，我们需要从技术上、经济上、心理上三个层面的数据支撑，确保项目的可执行性。回到效益函数上来：假设我们确定了需要调整因素，那么我们需要针对因素构建一个模型，确保因素在技术上、经济上、消费者心理上是可以被接受的。您提到的“ 再比如有的功能是肯定要上的，但领导会说，调研一下有没有必要，评估数据搞半个月，评估的结果是：可做。 实际上，该功能整个平台的用户都希望做，是没有必要耗费人力评估的，只要做就可以了。”那么实际上增加这个功能，也同样需要进行这样的评估，确保其起到的积极作用大于消极作用；带来的收益要高于其风险；所耗费的精力时间是值得优先处理的。案例：最近在指导一个电商团队做推广，其运营的成员在极力推广自己两款的独创产品，原因是：1、这两款产品是原创的，本身具备一定吸引力，转化率相对高；2、这两款产品是原创的，可以推动消费者对店铺的好感度；3、这两款产品库存深，需要清理。但如果电商的运营成员分析一下产品的利润率是否足以支撑产品的推广费用的话，就会发现整个ROI做下来不到0.3（投出1块钱，收入3毛钱），其利润率决定了这两款产品不适合做这种成本的推广。所以，有些事情看起来是非做不可，在某些角度看也是可行的，但通过数据分析，我们可以更本质地、更全面地评估事情的可执行性。最后，数据分析还有一点好处，那就是让非专业的人士，也可以直观地明白事情是否可行，毕竟“经验”的东西往往难以用来说服老板。

---

PM的数据分析窃以为分为3个部分：   1.数据监测：通过测度产品所表现的关键相应指标来评估产品目前的运行状况，并保障产品正常运行，如果有意外，第一时间发现，并分析原因；赘述一句，想象到的就不是意外，预案在此时通常很苍白，因此数据问责很有可能出现，PM也好，RD也好这时要冷静而不是抱怨。   2.数据探索：每一个PM都是贪心的，恨不得将所有可能测度到的数据全部让RD实现，这在效率和效果上都是个坏主意。因此，PM需要在让RD把所有数据都跑出来之前，做足探索，哪些数据有效，哪些没什么用，往往PM在处理项目时，评定优先级，而在数据上往往过于急躁。   2需要不断迭代，来完善1数据监测。   3.数据假设与数据挖掘：通过数据来推翻而不是论证项目的假设。基于数据获取更多的知识，甚至能够通过数据自学习，形成良性的自我成长。   这3个部分能杀掉PM大量的时间，故形成BI体系，作为团队内部的知识分享平台，至关重要。

---

数据的重要性不可否认，是客观评判的方法之一，证明一个想法不是拍脑门出来。   1.数据要恰当地利用。通常，老板是最喜欢看数据和听故事的人。你没办法理解为什么花一堆时间去证明一个显然易见的问题，是因为你没有站在他的角度来看。老板总是多虑的，需要数据作为有效支撑。但同时，每天又有多少PM总是拿着数据和开发吵架的事情发生。可见，数据并不是在哪个场合都那么的具有说服力的。关键是要搞清楚沟通的对象和对方的顾虑，这些数据是否足以解决对方心中的问题。  2.数据分析要一针见血  数据是如此的海量，但是我们需要的仅仅是一针见血的数据。所以，找数据以前，先弄清楚两点：你希望从数据中找到问题，还是你希望通过数据来证明你的观点，你要解决什么核心问题，关键的是要证明什么？而证明的关键数据是？有方向地找数据才更有效率。  3.数据需要包装  数据总是冷冰冰没有感情，罗列数据有赤裸裸的感觉，数据分析不是说把数据摊出来就解决了问题，更重要的是怎么包装好数据并且使其可视化，一眼就可以看出解决什么问题的数据图表，才是体现了分析过程和思考过程的数据分析，经过包装的数据，有时候很诱人。

---

第一本 《用数字说话》

第二本《谁说图表不会说谎》

---

统计数据、相关性与因果关系

    在去年10月份的数学文化节期间，我去听了好几次讲座，其中有一些讲的相当精彩。时间过得好快，转眼间又是一年了，如果不是Wind牛发短信问我去不去听讲座，我估计今年数学文化节过了都还想不起这档子事。于是和Wind牛跑去二教309，听了一场叫做《从数据中挖掘因果关系》的讲座。这个题目是很有趣的：数据本身并不说谎，难就难在我们如何从中挖掘出正确的信息。当我们讨论数据时，我们讲的最多的是数据的相关性，而我们希望得到的则是事件之间的因果联系；但事实往往是复杂的，统计数据有相关性并不意味着两个事件具有因果联系，而具有因果联系的两件事从统计数据上看有时也并不相关。
    对于前者，最简单的例子就是公鸡打鸣与太阳升起：公鸡打鸣与太阳升起总是同时发生，但这并不表示把全世界所有的公鸡都杀光了后太阳就升不起来了。统计发现，手指头越黄的人，得肺癌的比例越大。但事实上，手指的颜色和得肺癌的几率之间显然没有直接的因果联系。那么为什么统计数据会显示出相关性呢？这是因为手指黄和肺癌都是由吸烟造成的，由此造成了这两者之间产生了虚假的相关性。我们还可以质疑：根据同样的道理，我们又如何能从统计数据中得出吸烟会致癌的结论呢？要想知道吸烟与癌症之间究竟是否有因果联系的话，方法很简单：找一群人随机分成两组，规定一组抽烟一组不抽烟，过它十几年再把这一拨人找回来，数一数看是不是抽烟的那一组人患肺癌的更多一些。这个实验方法本身是无可挑剔的，但它太不道德了，因此我们只能考虑用自然观察法：选择一些本来都不吸烟的健康人进行跟踪观察，然后呢，过段时间这一拨人里总会出现一些失意了堕落了犯上烟瘾的人，于是随着时间的流逝这帮人自然而然地分成了可供统计观察的两组人。注意，这里“是否吸烟”这一变量并不是随机化得来的，它并没有经过人为的干预，而是自然区分出来的。这是一个致命的缺陷！统计结果表明，犯上烟瘾的那些人得肺癌的几率远远高于其他人。这真的能够说明吸烟致癌吗？仔细想想你会发现这当然不能！原因恰似黄手指与肺癌一例：完全有可能是某个第三方变量同时对“爱吸烟”和“患肺癌”产生影响。1957年，Fisher提出了两个备选理论：癌症引起吸烟（烟瘾是癌症早期的一个症状），或者存在某种基因能够同时引起癌症和烟瘾。
    有虚假的相关性数据，就有虚假的独立性数据。“健康工人效应”是一个特别有意思的理论。调查发现，在铀矿工作的工人居然与其它人的寿命一样长（有时甚至更长）。这表明在铀矿工作对身体无害么？当然不是！其实，是因为去铀矿工作的工人都是经过精心挑选的身强体壮的人，他们的寿命本来就该长一些，正是因为去了铀矿工作才把他们的寿命拉低到了平均水平。这一有趣的细节导致了数据的伪独立性。类似地，有数据表明打太极拳的人和不打太极拳的人平均寿命相同。事实上呢，太极拳确实可以强身健体、延长寿命，但打太极拳的人往往是体弱多病的人，这一事实也给统计数据带来了虚假的独立性。


    现实中的统计数据往往会表现出一些更加诡异复杂的反常现象。Simpson悖论是统计学中最有名的悖论：各个局部表现都很好，合起来一看反而更差。统计学在药物实验中的应用相当广泛，每次推出一种新药，我们都需要非常谨慎地进行临床测试。但有时候，药物实验的结果会让人匪夷所思。假设现在我们有一种可以代替安慰剂的新药。统计数据表明，这种新药的效果并不比安慰剂好：

         有效   无效   总人数
新药      80    120     200
安慰剂   100    100     200

    简单算算就能看出，新药只对40%的人有效，而安慰剂则对50%的人有效。新药按理说应该更好啊，那问题出在哪里呢？是否是因为这种新药对某一类人有副作用？于是研究人员把性别因素考虑进来，将男女分开来统计：

       男性有效  男性无效  女性有效  女性无效
新药      35        15        45       105
安慰剂    90        60        10        40

    大家不妨实际计算一下：对于男性来说，新药对高达70%的人都有效，而安慰剂则只对60%的人有效；对于女性来说，新药对30%的人都有效，而安慰剂则只对20%的人有效。滑稽的一幕出现了：我们惊奇地发现，新药对男性更加有效，对女性也更加有效，但对整个人类则无效！

    这种怪异的事屡见不鲜。前几个月一个高中的师弟给我发短信，给了我两个大学的名字，问该填报哪个好。鉴于我目前的悲惨境遇，我非常认真地帮他查了一下两所大学的男女比例，并且很细致地将表格精确到了各个院系。然后呢，怪事出现了：A学校的每个院系的女生比例都比B学校的同院系要高，但合起来一看就比B学校的低。当然，进错了大学找不到MM是小事，大不了像我一样20岁了连初吻都还没有，拿出去丢丢人让别人笑话笑话就完事了；但医药研究需要的是极其精细的统计实验，稍微出点差错的话害死的可就不是一两个人了。上面的例子再次告诉我们，统计实验的“随机干预”有多么重要。从上面的数据里我们直接看到，这个实验的操作本身就有问题：新药几乎全是女的在用，男的则大都在用安慰剂。被试者的分组根本没有实现完全的随机化，这才导致了如此混乱的统计结果；不难设想，如果每种药物的使用者都是男女各占一半，上述的悖论也就不会产生了。当然，研究人员也不都是傻子，这么重大的失误一般还是不会发生的。问题很可能出在一些没人注意到的小细节上。比如说，实验的时候用粉色的瓶子装新药，用蓝色的瓶子装安慰剂，然后让被试人从中随机选一个来用。结果呢，MM喜欢粉色，选的都是新药；男的呢则大多选择了蓝瓶子，用的都是安慰剂。最后，新药和安慰剂都发完了，因此直到结果出来之前没有人会注意到这个微小的性别差异所带来的统计失误。
    当然，上面这个药物实验的例子并不是真实的，一看就知道那个数据是凑出来方便大家计算的。不过，永远不要以为这种戏剧性的事件不会发生。一本叫做《致命的药物》的书详细披露了20世纪美国的一次重大药害事件，其原因可以归结到药物实验上去。药物实验的时间是有限的，如果用死亡率作为唯一标准的话，估计每个药物实验都得观察个十几二十年才行。为此，科学家们想到了利用各种“中间变量”来替代死亡率这一指标。

    统计数据表明，抑制心律失常能够减少死亡率，而当时的药物实验明确表明该药物能有效地抑制心律失常。这些药物得到了FDA批准并成功上市，当时每年有20多万人服用这些药品，超过5万人因为服用这种药物而死亡。这个药物实验中蕴含的逻辑推理看似无懈可击，到底什么地方出错了呢？人们推测很可能是某个第三方变量的问题。我们不妨称这种情况为“中间变量悖论”。

               抑制心律失常           死亡率
              对照组  实验组     未抑制    抑制
无缺陷(70%)    0.02    0.99       0.02     0.01
有缺陷(30%)    0.98    0.79       1.00     0.02
---------------------------------------------------
期望值         0.38    0.93       0.314    0.013

    让我们假设存在一个第三方因素，例如基因问题。我们不妨暂时管它叫做“先天缺陷”。从上表中我们可以看到，实验组（使用新药的人）中有93%的人成功抑制了心律失常，远远高于什么都不做的人(38%)；同时，心律失常确实会导致31.4%的人心脏骤停而死，但抑制心律失常则把这个比率下降到1.3%。这似乎确实可以说明，新药能够有效降低死亡率。但引入第三方因素后，情况有了很大的改变。有先天缺陷的人，心律往往很正常，恐怖的是一旦无法抑制心律失常则必死无疑。真正要命的就是，这种药物会使那些有先天缺陷的人心律变得更差。在没有缺陷的那70%的人当中，用药后有99%的人能抑制心律失常，而这里面只有1%的人会死；同时，另外1%的人则无法抑制，其中又有2%的人会死亡；有先天缺陷的那30%的人就惨了，用药后抑制住心律失常的人反而下降到79%，其中有2%的人会死，而对于另外21%的人则必死无疑。计算表明，使用药物后死亡的人数竟然三倍于不使用药物时的情况！

   (0.7*0.99*0.01 + 0.7*0.01*0.02 + 0.3*0.79*0.02 + 0.3*0.21*1.00)
/ (0.7*0.02*0.01 + 0.7*0.98*0.02 + 0.3*0.98*0.02 + 0.3*0.02*1.00)
≈ 2.91

    可以看到，从数据中挖掘因果关系并不是那么简单的事。如何确定影响目标的事件，如何从数据中获取相关关系，怎样用最少的实验次数（控制最少的变量）为因果关系定向，这都是建立一个因果网络所需要考虑的因素。因果网络是一个很复杂的学问，前天的讲座里还提到了很多确定因果网络的算法，在这里我就不再多说了。
    
---

大家都知道，对于产品经理的岗位要求的能力还是比较多的，如果我们对这些能力，按照硬技能和软技能进行分类的话，就有且不止以下这些能力：

软技能：沟通能力、决策能力、逻辑分析能力、执行力、项目管理能力等；

硬技能（工具能力）：文档能力、Visio、Axure、Mindmanger等；

那么，今天，我们要再讨论讨论产品经理的另一种非常重要的能力---数据分析能力。

何为数据分析？

现在的软件开发，都讲究小而美，单点突破，快速迭代。那么我们在快速迭代时，就要用到数据分析，通过用户使用数据来分析产品的优缺点，甚至方向的正确与否。因此，数据分析，就是产品迭代时的眼镜和耳朵，产品经理也是通过数据分析，来说服开发做功能，说服老板投入资源。

数据分析的概念：

数据分析是指用适当的统计方法对收集来的大量第一手的资料和第二手的资料进行分析，以求最大化地开发数据资料的功能，发挥数据的作用。通过对数据的详细研究和概括总结以提取用户信息和形成结论。

数据分析使用场景列举：

1、如果某款游戏下载量高，注册率低；是否因为服务器登录问题或者注册流程繁琐，是否近期网络出现故障？

2、如果某款游戏数据一直良好，某段时间数据突然跌落；是否因为市场宣传力度减弱，是否因为用户生命周期上线，还是说其他精品的冲击？

我们必须了解的一点，是数据分析不在于数据本身，而在于分析的能力。数据只是参照物，只是标杆，分析才是行为，通过分析数据，我们发现问题的所在，再改进它。

需要分析哪些数据？

基础数据：下载量、激活量、新增用户量、活跃用户等；

社交产品：用户分布、用户留存（次日、3日、7日、次月、3月）等；

电商：淘宝指数、网站流量、跳出率、页面访问深度等；

内容类：内容转化率（内容下载量/内容浏览量）、留存量；

工具类：功能点击量、应用商城排名；

其他：竞品数据（下载、激活等）；

数据分析的工具：

1、第三方数据分析工具，如友盟，可快速接入，节省成本，比较适合创业型公司及刚上线的产品，但是无法对关键数据在突发异样时进行跟踪；

2、自己开发的数据分析工具，可以对每个数据进行实时跟踪，并快速做出产品的调整，需要足够的开发人员及成本，比较适合大型公司或者成熟型产品；

如何进行数据分析？

对于这个问题，我想作为产品的工作人员，我们还不用达到数据分析师的高度，因此也不用说要先对数据建模，再对实际分析数据，看是否与模型吻合。但是，我们却需要要有一个产品数据分析的思路，这个思路可以这样展开：

1、我为什么分析？即就是明白，我分析的目的是什么，是寻找付费用户下降的原因？还是注册用户减少的原因？

2、我分析想要达到什么效果？是通过分析付费用户，找到问题，解决问题从而提升收入？

3、我该分析哪些数据才能达到这个效果？即需要什么数据才能达到分析的目的；

4、我又该如何采集这些数据？是直接通过第三方数据分析工具或者我们自己开发的工具就可获得？还是说要从数据库调取再交给程序猿哥哥？

5、我该如何整理这些数据？即我们常说的数据可视化，这样可以便于我们进行分析；

6、如何分析？即通过分析，找出问题的所在，给出你的结论；

7、怎么解决问题？给出你的解决方案；


---

著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：Xiaoyu Ma
链接：http://www.zhihu.com/question/27974418/answer/38965760
来源：知乎

大数据本身是个很宽泛的概念，Hadoop生态圈（或者泛生态圈）基本上都是为了处理超过单机尺度的数据处理而诞生的。你可以把它比作一个厨房所以需要的各种工具。锅碗瓢盆，各有各的用处，互相之间又有重合。你可以用汤锅直接当碗吃饭喝汤，你可以用小刀或者刨子去皮。但是每个工具有自己的特性，虽然奇怪的组合也能工作，但是未必是最佳选择。大数据，首先你要能存的下大数据。传统的文件系统是单机的，不能横跨不同的机器。HDFS（Hadoop Distributed FileSystem）的设计本质上是为了大量的数据能横跨成百上千台机器，但是你看到的是一个文件系统而不是很多文件系统。比如你说我要获取/hdfs/tmp/file1的数据，你引用的是一个文件路径，但是实际的数据存放在很多不同的机器上。你作为用户，不需要知道这些，就好比在单机上你不关心文件分散在什么磁道什么扇区一样。HDFS为你管理这些数据。存的下数据之后，你就开始考虑怎么处理数据。虽然HDFS可以为你整体管理不同机器上的数据，但是这些数据太大了。一台机器读取成T上P的数据（很大的数据哦，比如整个东京热有史以来所有高清电影的大小甚至更大），一台机器慢慢跑也许需要好几天甚至好几周。对于很多公司来说，单机处理是不可忍受的，比如微博要更新24小时热博，它必须在24小时之内跑完这些处理。那么我如果要用很多台机器处理，我就面临了如何分配工作，如果一台机器挂了如何重新启动相应的任务，机器之间如何互相通信交换数据以完成复杂的计算等等。这就是MapReduce / Tez / Spark的功能。MapReduce是第一代计算引擎，Tez和Spark是第二代。MapReduce的设计，采用了很简化的计算模型，只有Map和Reduce两个计算过程（中间用Shuffle串联），用这个模型，已经可以处理大数据领域很大一部分问题了。那什么是Map什么是Reduce？考虑如果你要统计一个巨大的文本文件存储在类似HDFS上，你想要知道这个文本里各个词的出现频率。你启动了一个MapReduce程序。Map阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的Pair（我这里把Map和Combine放在一起说以便简化）；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动Reduce处理。Reducer机器A将从Mapper机器收到所有以A开头的统计结果，机器B将收到B开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生Hash值以避免数据串化。因为类似X开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个Reducer都如上处理，你就得到了整个文件的词频结果。这看似是个很简单的模型，但很多算法都可以用这个模型描述了。Map＋Reduce的简单模型很黄很暴力，虽然好用，但是很笨重。第二代的Tez和Spark除了内存Cache之类的新feature，本质上来说，是让Map/Reduce模型更通用，让Map和Reduce之间的界限更模糊，数据交换更灵活，更少的磁盘读写，以便更方便地描述复杂算法，取得更高的吞吐量。有了MapReduce，Tez和Spark之后，程序员发现，MapReduce的程序写起来真麻烦。他们希望简化这个过程。这就好比你有了汇编语言，虽然你几乎什么都能干了，但是你还是觉得繁琐。你希望有个更高层更抽象的语言层来描述算法和数据处理流程。于是就有了Pig和Hive。Pig是接近脚本方式去描述MapReduce，Hive则用的是SQL。它们把脚本和SQL语言翻译成MapReduce程序，丢给计算引擎去计算，而你就从繁琐的MapReduce程序中解脱出来，用更简单更直观的语言去写程序了。有了Hive之后，人们发现SQL对比Java有巨大的优势。一个是它太容易写了。刚才词频的东西，用SQL描述就只有一两行，MapReduce写起来大约要几十上百行。而更重要的是，非计算机背景的用户终于感受到了爱：我也会写SQL！于是数据分析人员终于从乞求工程师帮忙的窘境解脱出来，工程师也从写奇怪的一次性的处理程序中解脱出来。大家都开心了。Hive逐渐成长成了大数据仓库的核心组件。甚至很多公司的流水线作业集完全是用SQL描述，因为易写易改，一看就懂，容易维护。自从数据分析人员开始用Hive分析数据之后，它们发现，Hive在MapReduce上跑，真鸡巴慢！流水线作业集也许没啥关系，比如24小时更新的推荐，反正24小时内跑完就算了。但是数据分析，人们总是希望能跑更快一些。比如我希望看过去一个小时内多少人在充气娃娃页面驻足，分别停留了多久，对于一个巨型网站海量数据下，这个处理过程也许要花几十分钟甚至很多小时。而这个分析也许只是你万里长征的第一步，你还要看多少人浏览了跳蛋多少人看了拉赫曼尼诺夫的CD，以便跟老板汇报，我们的用户是猥琐男闷骚女更多还是文艺青年／少女更多。你无法忍受等待的折磨，只能跟帅帅的工程师蝈蝈说，快，快，再快一点！于是Impala，Presto，Drill诞生了（当然还有无数非著名的交互SQL引擎，就不一一列举了）。三个系统的核心理念是，MapReduce引擎太慢，因为它太通用，太强壮，太保守，我们SQL需要更轻量，更激进地获取资源，更专门地对SQL做优化，而且不需要那么多容错性保证（因为系统出错了大不了重新启动任务，如果整个处理时间更短的话，比如几分钟之内）。这些系统让用户更快速地处理SQL任务，牺牲了通用性稳定性等特性。如果说MapReduce是大砍刀，砍啥都不怕，那上面三个就是剔骨刀，灵巧锋利，但是不能搞太大太硬的东西。这些系统，说实话，一直没有达到人们期望的流行度。因为这时候又两个异类被造出来了。他们是Hive on Tez / Spark和SparkSQL。它们的设计理念是，MapReduce慢，但是如果我用新一代通用计算引擎Tez或者Spark来跑SQL，那我就能跑的更快。而且用户不需要维护两套系统。这就好比如果你厨房小，人又懒，对吃的精细程度要求有限，那你可以买个电饭煲，能蒸能煲能烧，省了好多厨具。上面的介绍，基本就是一个数据仓库的构架了。底层HDFS，上面跑MapReduce／Tez／Spark，在上面跑Hive，Pig。或者HDFS上直接跑Impala，Drill，Presto。这解决了中低速数据处理的要求。那如果我要更高速的处理呢？如果我是一个类似微博的公司，我希望显示不是24小时热博，我想看一个不断变化的热播榜，更新延迟在一分钟之内，上面的手段都将无法胜任。于是又一种计算模型被开发出来，这就是Streaming（流）计算。Storm是最流行的流计算平台。流计算的思路是，如果要达到更实时的更新，我何不在数据流进来的时候就处理了？比如还是词频统计的例子，我的数据流是一个一个的词，我就让他们一边流过我就一边开始统计了。流计算很牛逼，基本无延迟，但是它的短处是，不灵活，你想要统计的东西必须预先知道，毕竟数据流过就没了，你没算的东西就无法补算了。因此它是个很好的东西，但是无法替代上面数据仓库和批处理系统。还有一个有些独立的模块是KV Store，比如Cassandra，HBase，MongoDB以及很多很多很多很多其他的（多到无法想象）。所以KV Store就是说，我有一堆键值，我能很快速滴获取与这个Key绑定的数据。比如我用身份证号，能取到你的身份数据。这个动作用MapReduce也能完成，但是很可能要扫描整个数据集。而KV Store专用来处理这个操作，所有存和取都专门为此优化了。从几个P的数据中查找一个身份证号，也许只要零点几秒。这让大数据公司的一些专门操作被大大优化了。比如我网页上有个根据订单号查找订单内容的页面，而整个网站的订单数量无法单机数据库存储，我就会考虑用KV Store来存。KV Store的理念是，基本无法处理复杂的计算，大多没法JOIN，也许没法聚合，没有强一致性保证（不同数据分布在不同机器上，你每次读取也许会读到不同的结果，也无法处理类似银行转账那样的强一致性要求的操作）。但是丫就是快。极快。每个不同的KV Store设计都有不同取舍，有些更快，有些容量更高，有些可以支持更复杂的操作。必有一款适合你。除此之外，还有一些更特制的系统／组件，比如Mahout是分布式机器学习库，Protobuf是数据交换的编码和库，ZooKeeper是高一致性的分布存取协同系统，等等。有了这么多乱七八糟的工具，都在同一个集群上运转，大家需要互相尊重有序工作。所以另外一个重要组件是，调度系统。现在最流行的是Yarn。你可以把他看作中央管理，好比你妈在厨房监工，哎，你妹妹切菜切完了，你可以把刀拿去杀鸡了。只要大家都服从你妈分配，那大家都能愉快滴烧菜。你可以认为，大数据生态圈就是一个厨房工具生态圈。为了做不同的菜，中国菜，日本菜，法国菜，你需要各种不同的工具。而且客人的需求正在复杂化，你的厨具不断被发明，也没有一个万用的厨具可以处理所有情况，因此它会变的越来越复杂。

---

著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：夏神经
链接：http://www.zhihu.com/question/27974418/answer/38946495
来源：知乎

Hadoop生态系统的图谱，详细的列举了Hadoop这个生态系统中出现的各种数据工具。 这一切，都起源自Web数据爆炸时代的来临 数据抓取系统 － Nutch 海量数据怎么存，当然是用分布式文件系统 － HDFS数据怎么用呢，分析，处理 MapReduce框架，让你编写代码来实现对大数据的分析工作 非结构化数据（日志）收集处理 － fuse,webdav, chukwa, flume, Scribe 数据导入到HDFS中，至此RDBSM也可以加入HDFS的狂欢了 － Hive, sqoop MapReduce太麻烦，好吧，让你用熟悉的方式来操作Hadoop里的数据 – Pig, Hive, Jaql 让你的数据可见 － drilldown, Intellicus用高级语言管理你的任务流 – oozie, Cascading Hadoop当然也有自己的监控管理工具 – Hue, karmasphere, eclipse plugin, cacti, ganglia 数据序列化处理与任务调度 – Avro, Zookeeper 更多构建在Hadoop上层的服务 – Mahout, Elastic map Reduce OLTP存储系统———————————————— hadoop vs spark Hadoop:分布式批处理计算，强调批处理，常用于数据挖掘、分析 Spark:是一个基于内存计算的开源的集群计算系统，目的是让数据分析更加快速, Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。 尽管创建 Spark 是为了支持分布式数据集上的迭代作业，但是实际上它是对 Hadoop 的补充，可以在 Hadoop 文件系统中并行运行。通过名为Mesos的第三方集群框架可以支持此行为。Spark 由加州大学伯克利分校 AMP 实验室 (Algorithms,Machines,and People Lab) 开发，可用来构建大型的、低延迟的数据分析应用程序。 虽然 Spark 与 Hadoop 有相似之处，但它提供了具有有用差异的一个新的集群计算框架。首先，Spark 是为集群计算中的特定类型的工作负载而设计，即那些在并行操作之间重用工作数据集（比如机器学习算法）的工作负载。为了优化这些类型的工作负载，Spark 引进了内存集群计算的概念，可在内存集群计算中将数据集缓存在内存中，以缩短访问延迟. 在大数据处理方面相信大家对hadoop已经耳熟能详，基于GoogleMap/Reduce来实现的Hadoop为开发者提供了map、reduce原语，使并行批处理程序变得非常地简单和优美。Spark提供的数据集操作类型有很多种，不像Hadoop只提供了Map和Reduce两种操作。比如map,filter, flatMap,sample, groupByKey, reduceByKey, union,join, cogroup,mapValues, sort,partionBy等多种操作类型，他们把这些操作称为Transformations。同时还提供Count,collect, reduce, lookup, save等多种actions。这些多种多样的数据集操作类型，给上层应用者提供了方便。各个处理节点之间的通信模型不再像Hadoop那样就是唯一的Data Shuffle一种模式。用户可以命名，物化，控制中间结果的分区。————————————————python 及其它Python和hadoop没啥关系，做数据分析时可以和R语言类比，实现streaming时可以和php ruby perl shell awk类比。Bigtable来自谷歌，对应hadoop里的HBaseGFS来自谷歌，对应hadoop里的HDFS……所以你看，搞懂hadoop是多么重要


